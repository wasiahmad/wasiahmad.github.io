---
title: "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla"
collection: publications
Authors: 'Abhik Bhattacharjee<sup>*</sup>, Tahmid Hasan<sup>*</sup>, <b>Wasi Uddin Ahmad</b>, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, and Rifat Shahriyar.'
date: 04/2022
venue: 'Findings of the ACL: NAACL'
paperurl: 'https://arxiv.org/abs/2101.00204'
codeurl: 'https://github.com/csebuetnlp/banglabert'
excerpt: ''
---
---
<a href='https://arxiv.org/pdf/2101.00204.pdf' target="_blank">[Download Paper]</a><a href='https://github.com/csebuetnlp/banglabert' target="_blank">[Source Code]</a>

<p align="justify">
In this work, we introduce &lsquo;BanglaBERT&rsquo;, a BERT-based Natural Language Understanding (NLU) model pretrained in Bangla, a widely spoken yet 
  low-resource language in the NLP literature. To pretrain BanglaBERT, we collect 27.5 GB of Bangla pretraining data (dubbed &lsquo;Bangla2B+&rsquo;) 
  by crawling 110 popular Bangla sites. We introduce two new downstream task datasets on natural language inference and question answering and benchmark 
  on four diverse NLU tasks covering text classification, sequence labeling, and span prediction. In the process, we bring them under the first-ever 
  Bangla Language Understanding Evaluation (BangLUE) benchmark. BanglaBERT achieves state-of-the-art results outperforming multilingual and monolingual 
  models. We are making the BanglaBERT model, the new datasets, and a leaderboard publicly available at 
  <a href='https://github.com/csebuetnlp/banglabert' target="_blank">https://github.com/csebuetnlp/banglabert</a> to advance Bangla NLP.
</p>
