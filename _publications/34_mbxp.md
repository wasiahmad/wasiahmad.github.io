---
title: "Multi-lingual Evaluation of Code Generation Models"
collection: publications
Authors: 'Nihal Jain<sup>*</sup>, Dejiao Zhang<sup>*</sup>, <b>Wasi Uddin Ahmad</b><sup>*</sup>, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma, and Bing Xiang.'
date: 10/2022
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2210.14868'
codeurl: 'https://github.com/amazon-science/mbxp-exec-eval'
excerpt: ''
---
---
<a href='https://arxiv.org/pdf/2210.14868.pdf' target="_blank">[Download Paper]</a><a href='https://github.com/amazon-science/mbxp-exec-eval' target="_blank">[Source Code]</a>

<p align="justify">
We present MBXP, an execution-based code completion benchmark in 10+ programming languages. This collection of datasets is generated by our conversion 
  framework that translates prompts and test cases from the original MBPP dataset to the corresponding data in a target language. Based on this benchmark, 
  we are able to evaluate code generation models in a multi-lingual fashion, and in particular discover generalization ability of language models on 
  out-of-domain languages, advantages of large multi-lingual models over mono-lingual, benefits of few-shot prompting, and zero-shot translation abilities. 
  In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages. 
  These solutions can be used for other code-related evaluations such as insertion-based, summarization, or code translation tasks where we demonstrate 
  results and release as part of our benchmark.
</p>
