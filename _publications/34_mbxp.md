---
title: "Multi-lingual Evaluation of Code Generation Models"
collection: publications
Authors: 'Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, <b>Wasi Uddin Ahmad</b>, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang.'
date: 01/2023
venue: 'ICLR'
paperurl: 'https://arxiv.org/abs/2210.14868'
codeurl: 'https://github.com/amazon-science/mbxp-exec-eval'
excerpt: ''
---
---
<a href='https://arxiv.org/pdf/2210.14868.pdf' target="_blank">[Download Paper]</a><a href='https://github.com/amazon-science/mbxp-exec-eval' target="_blank">[Source Code]</a>

<p align="justify">
We present MBXP, an execution-based code completion benchmark in 10+ programming languages. This collection of datasets is generated by our conversion 
  framework that translates prompts and test cases from the original MBPP dataset to the corresponding data in a target language. Based on this benchmark, 
  we are able to evaluate code generation models in a multi-lingual fashion, and in particular discover generalization ability of language models on 
  out-of-domain languages, advantages of large multi-lingual models over mono-lingual, benefits of few-shot prompting, and zero-shot translation abilities. 
  In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages. 
  These solutions can be used for other code-related evaluations such as insertion-based, summarization, or code translation tasks where we demonstrate 
  results and release as part of our benchmark.
</p>
