---
title: "ContraCLM: Contrastive Learning For Causal Language Model"
collection: publications
Authors: 'Nihal Jain<sup>*</sup>, Dejiao Zhang<sup>*</sup>, <b>Wasi Uddin Ahmad</b><sup>*</sup>, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma, and Bing Xiang.'
date: 05/2023
venue: 'ACL'
paperurl: 'https://aclanthology.org/2023.acl-long.355'
codeurl: 'https://github.com/amazon-science/ContraCLM'
excerpt: ''
---
---
<a href='https://aclanthology.org/2023.acl-long.355/' target="_blank">[Download Paper]</a><a href='https://github.com/amazon-science/ContraCLM' target="_blank">[Source Code]</a>

<p align="justify">
Despite exciting progress in large-scale language generation, the expressiveness of its representations is severely limited by the 
  anisotropy issue where the hidden representations are distributed into a narrow cone in the vector space. To address this issue, 
  we present ContraGen, a novel contrastive learning framework to improve the representation with better uniformity and discrimination. 
  We assess ContraGen on a wide range of downstream tasks in natural and programming languages. We show that ContraGen can effectively enhance 
  both uniformity and discrimination of the representations and lead to the desired improvement on various language understanding tasks where 
  discriminative representations are crucial for attaining good performance. Specifically, we attain 44% relative improvement on the Semantic 
  Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of the representations, ContraGen 
  also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.
</p>
