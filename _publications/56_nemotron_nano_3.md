---
title: "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning"
collection: publications
Authors: 'NVIDIA'
date: 12/2025
venue: 'NVIDIA Research Blog'
paperurl: 'https://research.nvidia.com/labs/nemotron/Nemotron-3'
modelurl: 'https://huggingface.co/collections/nvidia/nvidia-nemotron-v3'
excerpt: ''
---
---
<a href='https://research.nvidia.com/labs/nemotron/files/NVIDIA-Nemotron-3-Nano-Technical-Report.pdf' target="_blank">[Download Paper]</a><a href='https://huggingface.co/collections/nvidia/nvidia-nemotron-v3' target="_blank">[Download Models]</a>
<p align="justify">
We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid MambaTransformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens 
  over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less 
  than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS20B and Qwen3-30B-A3B-Thinking-2507, while also being more 
  accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained 
  Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.
</p>
