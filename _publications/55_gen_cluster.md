---
title: "Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models"
collection: publications
Authors: 'Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, <b>Wasi Uddin Ahmad</b>, Somshubra Majumdar, Vahid Noroozi, and Boris Ginsburg.'
date: 10/2025
venue: 'NeurIPS 2025 Fourth Workshop on Deep Learning for Code'
paperurl: 'https://arxiv.org/abs/2510.14232'
excerpt: ''
---
---
<a href='https://arxiv.org/pdf/2510.14232' target="_blank">[Download Paper]</a>
<p align="justify">
Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). 
  The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has 
  become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold 
  medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant 
  challenge. In this paper, we present \gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance 
  using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently 
  explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales 
  consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold 
  medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of 
  reasoning in LLMs.
</p>
