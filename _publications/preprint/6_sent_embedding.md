---
title: "Learning Robust, Transferable Sentence Representations for Text Classification"
collection: publications
Authors: '<b>Wasi Ahmad</b>, Xueying Bai, Nanyun Peng, and Kai-Wei Chang.'
date: 09/2018
venue: 'arXiv'
paperurl: 'https://wasiahmad.github.io/files/publications/2018/transferable_representation.pdf'
codeurl: 'https://github.com/wasiahmad/transferable_sent2vec'
excerpt: ''
---
---
<a href='https://wasiahmad.github.io/files/publications/2018/transferable_representation.pdf' target="_blank">[Download Paper]</a>

<p align="justify">
Despite deep recurrent neural networks (RNNs) demonstrate strong performance in text classification, training RNN models are 
often expensive and requires an extensive collection of annotated data which may not be available. To overcome the data 
limitation issue, existing approaches leverage either pre-trained word embedding or sentence representation to lift the 
burden of training RNNs from scratch. In this paper, we show that jointly learning sentence representations from multiple 
text classification tasks and combining them with pre-trained word-level and sentence level encoders result in robust 
sentence representations that are useful for transfer learning. Extensive experiments and analyses using a wide range of 
transfer and linguistic tasks endorse the effectiveness of our approach.
</p>
